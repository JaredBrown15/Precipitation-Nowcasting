{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b11c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20273 events matching {'vil'}\n"
     ]
    }
   ],
   "source": [
    "import h5py # needs conda/pip install h5py\n",
    "import os\n",
    "os.chdir(r'C:\\Users\\jared\\OneDrive\\Documents\\School\\Thesis')\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'C:\\users\\jared\\anaconda3\\lib\\site-packages')\n",
    "# import command_file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import spear as sp\n",
    "import numpy as np\n",
    "from pathlib import Path  \n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import gruHelperScript as ghs\n",
    "# import lstmHelperScript as lhs\n",
    "# import encoderDecoderHelperScript as edhs\n",
    "\n",
    "DATA_PATH    = 'D:\\SEVIR Data\\data'\n",
    "CATALOG_PATH = 'D:\\SEVIR Data/CATALOG.csv'\n",
    "\n",
    "# Read catalog\n",
    "catalog = pd.read_csv(CATALOG_PATH,parse_dates=['time_utc'],low_memory=False)\n",
    "\n",
    "# Desired image types\n",
    "img_types = set(['vil'])\n",
    "\n",
    "# Group by event id, and filter to only events that have all desired img_types\n",
    "events = catalog.groupby('id').filter(lambda x: img_types.issubset(set(x['img_type']))).groupby('id')\n",
    "event_ids = list(events.groups.keys())\n",
    "print('Found %d events matching' % len(event_ids),img_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ccfb04",
   "metadata": {},
   "source": [
    "# CHANGE FOR LOOP BELOW TO DO ALL FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a238da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20273 events matching {'vil'}\n"
     ]
    }
   ],
   "source": [
    "import master_script_helper_functions as mshf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2a9714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maskingThreshold = [150, 75, 94, 113, 144, 81, 69, 138, 100, 50, 125, 106, 88, 56, 119, 131, 63]\n",
    "# numSTD = [1.8, 2.1, 1.5, 3, 1.3, 2.6, 1.6, 2.9, 2, 2.3, 1.9, 2.5, 1, 2.8, 1.4, 2.4, 1.1]\n",
    "\n",
    "# maskingThreshold = [46, 17, 24, 32, 96, 125, 82, 75, 68, 89, 118, 111, 103, 39, 10, 53, 60]\n",
    "# numSTD = [1.5, 0.5, 0.7, 1, 1.4, 0.5, 0.4,1.3, 0.8, 0.1, 1.2, 0.9, 0.6, 0.2, 1.1, 1.2, 0.3]\n",
    "\n",
    "# maskingThreshold = [58, 36, 41, 47, 98, 120, 86, 81, 75, 92, 114, 109, 103, 53, 30, 64, 69]\n",
    "# numSTD = [1.5, 0.8, 0.9, 1.1, 1.4, 0.8, 0.7, 1.4, 1, 0.5, 1.3, 1.1, 0.9, 0.6, 1.2, 1.3, 0.6]\n",
    "\n",
    "maskingThreshold = [50]\n",
    "numSTD = [1]\n",
    "# input_lengths = [8]\n",
    "# horizon_lengths = [1]\n",
    "\n",
    "# numSTD = [1, 1, 1, 1]\n",
    "# maskingThreshold = [114, 65, 50, 50]\n",
    "globalMeans = [];\n",
    "glogalSTDs = [];\n",
    "\n",
    "# for i in range(len(numSTD)):\n",
    "#     np_data, np_data_no_text = mshf.createParameters(maskingThreshold[i], numSTD[i])\n",
    "# [Binarization Threshold, numSTD] are the inputs above\n",
    "\n",
    "# maskingThreshold = [150, 75, 94, 113, 144, 81, 69, 138, 100, 50, 125, 106, 88, 56, 119, 131, 63]\n",
    "# numSTD = [1.8, 2.1, 1.5, 3, 1.3, 2.6, 1.6, 2.9, 2, 2.3, 1.9, 2.5, 1, 2.8, 1.4, 2.4, 1.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ccbfef",
   "metadata": {},
   "source": [
    "# Creating X and Y For and Pytorch Data Loaders for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a315a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createXY(dataset, input_length, horizon_length):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sample in dataset:\n",
    "        for i in range(len(sample) - input_length - horizon_length + 1):\n",
    "            x = torch.tensor(sample[i:i+input_length], dtype=torch.float32)\n",
    "            y = torch.tensor(sample[i+input_length:i+input_length+horizon_length], dtype=torch.float32)\n",
    "#             x_padded = pad_sequence(x, max_input_len, 0)\n",
    "#             y_padded = pad_sequence(y, max_horizon_len, 0)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10a9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataset(train, test):\n",
    "    stacked_train = np.array(np.vstack(train))\n",
    "    stacked_train = stacked_train.astype(np.float64)\n",
    "    means = np.mean(stacked_train, axis=0)\n",
    "    stds =  np.std(stacked_train, axis=0)\n",
    "    globalMeans = means;\n",
    "    globalSTDs = stds;\n",
    "    normalized_train = (stacked_train - means) / stds\n",
    "    train_return = normalized_train.reshape(-1,48,13)\n",
    "    \n",
    "    stacked_test = np.array(np.vstack(test))\n",
    "#     stacked_test = stacked_test.astype(np.float64)\n",
    "    normalized_test = (stacked_test - means) / stds\n",
    "    test_return = normalized_test.reshape(-1,48,13)\n",
    "    return train_return, test_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9180e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeZeroEvents(dataset):\n",
    "    print(dataset.shape)\n",
    "    \n",
    "    dataset = dataset.astype(np.float64)\n",
    "    valid_events_mask = ~np.any(np.isnan(dataset), axis=(1, 2))\n",
    "    # Use the mask to filter out the invalid events\n",
    "    filtered_data = dataset[valid_events_mask]\n",
    "    \n",
    "    new_set = []\n",
    "    for event in filtered_data:\n",
    "        if not(np.any(event[:, 0:2] < 3)):\n",
    "            new_set.append(event)\n",
    "    new_set = np.array(new_set)\n",
    "    new_set = new_set.astype(np.float64)\n",
    "#     valid_events_mask = ~np.any(np.isnan(new_set), axis=(1, 2))\n",
    "#     # Use the mask to filter out the invalid events\n",
    "#     filtered_data = new_set[valid_events_mask]\n",
    "    \n",
    "    print(new_set.shape)\n",
    "    return new_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee0dbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInputOutputs(dataset, input_length, output_lengths, rand_state, test_percent=0.2):\n",
    "    \"\"\"    \n",
    "    Inputs: Dataset, input_lengths, output_lengths (horizons), test_set_percentage (20% default)\n",
    "    \n",
    "    Removes zero events\n",
    "    Splits into train/test (80/20 ratio default)\n",
    "    Normalizes train, then normalizes test using train means\n",
    "    Creates dictionary of X/Y vectors for each input/output combo\n",
    "    Returns dictionary of X/Y vectors for both train and test set\n",
    "    \n",
    "    Returns: normalized dictionary of X/Y Vectors for every input/output combo\n",
    "    \"\"\"\n",
    "    dataset = removeZeroEvents(dataset)\n",
    "#     Split intro train/test with default 20% test and then normalize\n",
    "    train, test = train_test_split(dataset, test_size=test_percent, random_state=rand_state)\n",
    "    norm_train, norm_test = normalizeDataset(train, test)\n",
    "    \n",
    "    train_dict = {}\n",
    "    test_dict = {}\n",
    "    min_set_size = 20000\n",
    "#     Create dictionary with keys of (input_length, horizon_length)\n",
    "#     Each keys has a value with the X vectors as [\"X\"] and y vectors as [\"y\"]\n",
    "#     for input_length in input_lengths:\n",
    "    for horizon_length in horizon_lengths:\n",
    "        print(\"Creating input and output vectors for Input: \" + str(input_length) + \" Horizon: \" + str(horizon_length))\n",
    "        train_x, train_y = createXY(norm_train, input_length, horizon_length)\n",
    "        key = (input_length, horizon_length)  # Create a key for the combination\n",
    "        train_dict[key] = {\"X\": [], \"Y\": []}\n",
    "        train_dict[key][\"X\"].append(train_x)\n",
    "        train_dict[key][\"Y\"].append(train_y)\n",
    "\n",
    "        test_x, test_y = createXY(norm_test, input_length, horizon_length)\n",
    "#         key = (input_length, horizon_length)  # Create a key for the combination\n",
    "        test_dict[key] = {\"X\": [], \"Y\": []}\n",
    "        test_dict[key][\"X\"].append(test_x)\n",
    "        test_dict[key][\"Y\"].append(test_y)\n",
    "        if(len(train_x) < min_set_size):\n",
    "            min_set_size = len(train_x)\n",
    "    return train_dict, test_dict, min_set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c137e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X_data, Y_data, X_lengths, Y_lengths):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "        self.X_lengths = X_lengths\n",
    "        self.Y_lengths = Y_lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X_data[idx]\n",
    "        Y = self.Y_data[idx]\n",
    "        X_len = self.X_lengths[idx]\n",
    "        Y_len = self.Y_lengths[idx]\n",
    "        \n",
    "        return X, Y, X_len, Y_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19a6d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate_fn to pad X and unpack Y sequences\n",
    "def collate_fn(batch):\n",
    "    # Sort by length in descending order (for packing)\n",
    "    batch.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    X, Y, X_lengths, Y_lengths = zip(*batch)\n",
    "    \n",
    "    # Pad X sequences\n",
    "    X_padded = pad_sequence(X, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Stack Y sequences (no padding here, since Y is unpadded)\n",
    "    Y_padded = pad_sequence(Y, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    X_lengths = torch.tensor(X_lengths)\n",
    "    Y_lengths = torch.tensor(Y_lengths)\n",
    "\n",
    "    return X_padded, Y_padded, X_lengths, Y_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92c611a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimSets(dictionary, max_size, input_length):\n",
    "#     Equalize size of each input/horizon pairing in train dictionary (max set size is 1.5x smallest set size)\n",
    "    for horizon_length in horizon_lengths:\n",
    "        X = dictionary[(input_length, horizon_length)][\"X\"][0]\n",
    "        y = dictionary[(input_length, horizon_length)][\"Y\"][0]\n",
    "\n",
    "        if(len(X) > max_size):\n",
    "            random_indices = random.sample(range(len(X)), max_size)\n",
    "            # Slice the lists using the random indices\n",
    "            X_Sampled = [X[i] for i in random_indices]\n",
    "            y_Sampled = [y[i] for i in random_indices]\n",
    "\n",
    "            dictionary[(input_length, horizon_length)][\"X\"][0] = X_Sampled\n",
    "            dictionary[(input_length, horizon_length)][\"Y\"][0] = y_Sampled\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f047e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create merged X Y for train dataset\n",
    "def mergeAllSets(dictionary, input_len):\n",
    "    X_all_Train = []\n",
    "    Y_all_Train = []\n",
    "    # total = 0\n",
    "    train_x_lengths = []\n",
    "    train_y_lengths = []\n",
    "\n",
    "    for horizon_length in horizon_lengths:\n",
    "        # Get the data for the current (input_len, output_horizon) pair\n",
    "        key = (input_len, horizon_length)\n",
    "        X = dictionary[key][\"X\"][0]\n",
    "        Y = dictionary[key][\"Y\"][0]\n",
    "        X_all_Train = X_all_Train + X\n",
    "        Y_all_Train = Y_all_Train + Y\n",
    "        train_x_lengths = train_x_lengths + [input_len]*len(dictionary[key][\"X\"][0])\n",
    "        train_y_lengths = train_y_lengths + [horizon_length]*len(dictionary[key][\"Y\"][0])\n",
    "        \n",
    "    return X_all_Train, Y_all_Train, train_x_lengths, train_y_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae7e8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataLoader(X_all_Train, Y_all_Train, train_x_lengths, train_y_lengths):\n",
    "    training_torch_dataset = SeqDataset(X_all_Train, Y_all_Train, train_x_lengths, train_y_lengths)\n",
    "\n",
    "#     For testing purposes, take 20% of dataset\n",
    "    dataset_size = len(training_torch_dataset) \n",
    "    subset_size = int(0.2 * dataset_size)  # Taking 20% of the dataset\n",
    "    subset, _ = random_split(training_torch_dataset, [subset_size, dataset_size - subset_size])\n",
    "    \n",
    "    train_data_loader = DataLoader(training_torch_dataset, batch_size=256, shuffle=True, drop_last=True, \n",
    "                                   collate_fn=lambda batch: collate_fn(batch))\n",
    "    return train_data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1aee801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel():\n",
    "    criterion = nn.MSELoss()  # Assuming you are doing regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training model\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, (X_batch, Y_batch, X_lengths, Y_lengths) in enumerate(train_data_loader):  # Assuming a data loader is used\n",
    "\n",
    "            # Calculate percentage\n",
    "            percent_done = (i + 1) / len(train_data_loader) * 100\n",
    "            # Print progress\n",
    "            print(f\"Processing: {percent_done:.2f}% complete for epoch {epoch+1}\", end='\\r')\n",
    "\n",
    "\n",
    "            # Move the batch to the same device as the model\n",
    "            X_batch = X_batch.to(device)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "            X_lengths = X_lengths.to(device)\n",
    "            Y_lengths = Y_lengths.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            Y_pred = model(X_batch, X_lengths, Y_lengths.max())\n",
    "\n",
    "            # Compute loss on non-padded parts\n",
    "            trimmed_y_pred = [arr[n-1] for arr, n in zip(Y_pred, Y_lengths)]\n",
    "            trimmed_y_true = [arr[n-1] for arr, n in zip(Y_batch, Y_lengths)]\n",
    "            trimmed_y_pred_tensor = torch.stack(trimmed_y_pred)\n",
    "            trimmed_y_true_tensor = torch.stack(trimmed_y_true)\n",
    "\n",
    "        #         print(trimmed_y_pred[0])\n",
    "        #         print(trimmed_y_true[0])\n",
    "            loss = criterion(trimmed_y_pred_tensor, trimmed_y_true_tensor)\n",
    "#             loss = criterion(trimmed_y_pred_tensor[:,:-5], trimmed_y_true_tensor[:,:-5])\n",
    "            train_loss.append(loss)\n",
    "            # Zero the gradients, perform a backward pass, and update the weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print loss\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96785173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class AutoregressiveLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.2):\n",
    "        super(AutoregressiveLSTMWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer for processing inputs\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, \n",
    "                             num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer to project hidden states to output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Attention Layer\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths, output_length):\n",
    "        # Pack the padded sequence (for training only)\n",
    "        packed_input = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass the packed input through LSTM\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
    "        \n",
    "        # Unpack the output (for training only, not used in autoregressive generation)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        # Now use hidden state to start autoregressive generation\n",
    "        next_input = output[:, -1, :].unsqueeze(1)  # Last hidden state as input\n",
    "        # Initialize hidden and cell states\n",
    "        next_input = self.fc(next_input.squeeze(1)).unsqueeze(1)  # Shape: [batch_size, 1, input_size]\n",
    "\n",
    "        hidden = hidden  # [num_layers, batch_size, hidden_size]\n",
    "        cell = cell  # [num_layers, batch_size, hidden_size]\n",
    "        \n",
    "        outputs = []\n",
    "        past_predictions = []  # Store past predictions for attention\n",
    "        \n",
    "        for t in range(output_length):\n",
    "            # LSTM generates a new output\n",
    "#             print(\"LSTM INPUT SIZE: \" + str(x.shape))\n",
    "            output_lstm, (hidden, cell) = self.lstm(next_input, (hidden, cell))\n",
    "#             print(\"OUTPUT_LSTM SIZE: \" + str(output_lstm.shape))\n",
    "\n",
    "            # Use attention on past predictions\n",
    "            if past_predictions:\n",
    "                past_preds = torch.stack(past_predictions, dim=1)  # [batch_size, t, output_size]\n",
    "                attn_weights = F.softmax(self.attn(past_preds), dim=1)  # Attention weights for past preds\n",
    "                weighted_past_preds = torch.sum(attn_weights * past_preds, dim=1)  # Weighted sum of past predictions\n",
    "#                 print(\"WEIGHTED PAST PREDS: \" + str(weighted_past_preds.shape))\n",
    "                output_lstm = output_lstm + weighted_past_preds.squeeze(1)  # Combine with new LSTM output\n",
    "#                 print(\"OUTPUT_LSTM SIZE w attn: \" + str(output_lstm.shape))\n",
    "                \n",
    "            # Fully connected layer for final prediction\n",
    "            output = self.fc(output_lstm.squeeze(1))  # [batch_size, output_size]\n",
    "            outputs.append(output)\n",
    "            \n",
    "            # Save the current prediction for future attention\n",
    "            past_predictions.append(output_lstm.unsqueeze(1))  # Add to past predictions list\n",
    "            \n",
    "            # Use output as next input for autoregressive generation\n",
    "            next_input = output.unsqueeze(1)  # This will be the sequence input for next timestep\n",
    "#             print(\"OUTPUT SIZE: \" + str(output.shape))\n",
    "\n",
    "        return torch.stack(outputs, dim=1)  # Stack to form a sequence of shape [batch_size, output_length, output_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a8dde64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTestSetDataloader(key):\n",
    "    current_x_test = test_dataset[key][\"X\"][0]\n",
    "    current_y_test = test_dataset[key][\"Y\"][0]\n",
    "    current_x_test_lengths = [key[0]] * len(current_x_test)\n",
    "    current_y_test_lengths = [key[1]] * len(current_y_test)\n",
    "    current_test_dataset = SeqDataset(current_x_test, current_y_test, current_x_test_lengths, current_y_test_lengths)\n",
    "    test_data_loader = DataLoader(current_test_dataset, batch_size=128, shuffle=True, drop_last=True, collate_fn=lambda batch: collate_fn(batch))\n",
    "    return test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89f0b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2ByColumn(y_true, y_pred):\n",
    "    # Calculate the mean of y_true along each column\n",
    "    y_mean = np.mean(y_true, axis=0)\n",
    "    # Calculate SS_tot and SS_res for each column\n",
    "    ss_tot = np.sum((y_true - y_mean) ** 2, axis=0)\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2, axis=0)\n",
    "    # Calculate R^2 for each column\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    rounded_r2 = [np.round(x, 4) for x in r2]\n",
    "    return rounded_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7491567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def globalR2(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true, axis=0)) ** 2)\n",
    "    r2_global = 1 - (ss_res / ss_tot)\n",
    "    return r2_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2607a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel():\n",
    "    r2_dict = {}\n",
    "    for key in test_dataset:\n",
    "        print(\"KEYS IN TEST DATASET\")\n",
    "        print(key)\n",
    "        test_data_loader = makeTestSetDataloader(key)\n",
    "        predictions = []\n",
    "        y_true = []\n",
    "        for i, (X_batch, Y_batch, X_lengths, Y_lengths) in enumerate(test_data_loader):\n",
    "            with torch.no_grad():\n",
    "                batch_predictions = model(X_batch.to(device), X_lengths.to(device), Y_lengths.max().to(device))\n",
    "\n",
    "            numpy_batch_predictions = batch_predictions.cpu().numpy()\n",
    "    #         print(numpy_batch_predictions[0,:,:])\n",
    "            numpy_batch_predictions = numpy_batch_predictions[:, -1, :] # Take only the final frame for computing r^2\n",
    "\n",
    "    #         print(numpy_batch_predictions.shape)\n",
    "            predictions.append(numpy_batch_predictions)\n",
    "            batch_y_true = Y_batch[:, -1, :] # Take only the final frame for computing r^2\n",
    "            y_true.append(batch_y_true.cpu().numpy())\n",
    "\n",
    "        predictions_array = np.array(predictions)\n",
    "    #     print(\"INPUT LENGTH: \" + str(int(X_lengths[0])))\n",
    "    #     print(\"HORIZON LENGTH: \" + str(int(Y_lengths[0])))\n",
    "    #     print(\"Predictions Length: \" + str(predictions_array.shape))\n",
    "\n",
    "    #     predictions_array = predictions_array.reshape(-1, Y_lengths[0], output_size)\n",
    "        predictions_array = predictions_array.reshape(-1, 1, output_size)\n",
    "        predictions_array = np.squeeze(predictions_array)\n",
    "    #     print(\"Number of Samples: \" + str(predictions_array.shape[0]) + \"\\n\")\n",
    "\n",
    "        y_true = np.array(y_true)\n",
    "    #     y_true = y_true.reshape(-1, Y_lengths[0], output_size)\n",
    "        y_true = y_true.reshape(-1, 1, output_size)\n",
    "        y_true = np.squeeze(y_true)\n",
    "    #     print(\"y_true Length: \" + str(y_true.shape))\n",
    "\n",
    "        r2_column = r2ByColumn(y_true, predictions_array)\n",
    "        r2_first8 = globalR2(y_true[:,:-5], predictions_array[:,:-5])\n",
    "        r2_global = globalR2(y_true, predictions_array)\n",
    "\n",
    "        r2_dict[key] = {\"first8\": r2_first8, \"global\": r2_global}\n",
    "    return r2_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49ba9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveR2Dict(mT, nstd, input_length, splitn):\n",
    "    np_path = \"C:\\\\Users\\\\jared\\\\OneDrive\\\\Documents\\\\School\\\\Thesis\\\\Final Results\\\\threshold_\" + str(mT) + \"_nSTD_\" + str(nstd) + \"\\\\input_\" + str(input_length) + \"\\\\train_test_split_\" + str(splitn) + \"_r2_dict.pkl\"\n",
    "\n",
    "    # Extract the directory from the path (so we can create it)\n",
    "    directory = os.path.dirname(np_path)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    try:\n",
    "        print(\"Creating Directory for R2\")\n",
    "        os.makedirs(directory, exist_ok=True)  # exist_ok=True to prevent error if directory already exists\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating the directory: {e}\")\n",
    "\n",
    "    # Save the dictionary to the file using pickle\n",
    "    with open(np_path, 'wb') as f:\n",
    "        pickle.dump(r2s, f)  # Save the dictionary (r2s) into the .pkl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106f230",
   "metadata": {},
   "source": [
    "# DEFINING GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cd4f7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import random\n",
    "from itertools import product\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "train_loss = []\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f14d6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Params\n",
    "input_size = 13  # Feature dimension for each time step in X\n",
    "output_size = 13  # Feature dimension for each time step in Y (same as input size)\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d2f550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Params\n",
    "# maskingThreshold = [150, 75, 94, 113, 144, 81, 69, 138, 100, 50, 125, 106, 88, 56, 119, 131, 63]\n",
    "# numSTD = [1.8, 2.1, 1.5, 3, 1.3, 2.6, 1.6, 2.9, 2, 2.3, 1.9, 2.5, 1, 2.8, 1.4, 2.4, 1.1]\n",
    "# input_lengths = [8, 2, 4, 5, 18, 24, 15, 14, 13, 17, 23, 21, 20, 7, 1, 10, 11]\n",
    "# horizon_lengths = [1, 6, 12, 24]\n",
    "\n",
    "\n",
    "# maskingThreshold = [46, 17, 24, 32, 96, 125, 82, 75, 68, 89, 118, 111, 103, 39, 10, 53, 60]\n",
    "# numSTD = [1.5, 0.5, 0.7, 1, 1.4, 0.5, 0.4,1.3, 0.8, 0.1, 1.2, 0.9, 0.6, 0.2, 1.1, 1.2, 0.3]\n",
    "# input_lengths = [12, 13, 2, 5, 7, 6, 15, 12, 8, 4, 3, 14, 11, 9, 10, 1, 5]\n",
    "# horizon_lengths = [1, 6, 12, 24]\n",
    "\n",
    "# maskingThreshold = [58, 36, 41, 47, 98, 120, 86, 81, 75, 92, 114, 109, 103, 53, 30, 64, 69]\n",
    "# numSTD = [1.5, 0.8, 0.9, 1.1, 1.4, 0.8, 0.7, 1.4, 1, 0.5, 1.3, 1.1, 0.9, 0.6, 1.2, 1.3, 0.6]\n",
    "# input_lengths = [13, 14, 4, 7, 8, 8, 15, 12, 9, 5, 5, 14, 11, 10, 11, 3, 6]\n",
    "# horizon_lengths = [1, 6, 12, 24]\n",
    "\n",
    "\n",
    "# maskingThreshold = [1, 1, 1, 1, 1, 1, 1, 1]\n",
    "# numSTD = [114, 65, 50, 50]\n",
    "# input_lengths = [1, 2, 4, 8, 16, 24]\n",
    "# horizon_lengths = [1, 6, 12, 24]\n",
    "\n",
    "\n",
    "# maskingThreshold = [114,  65,   50,   50]\n",
    "# numSTD =           [1,    1,    1,    1]\n",
    "# input_lengths =    [8,    4,    9,    13]\n",
    "# horizon_lengths =  [1,    6,    12,   24]\n",
    "\n",
    "maskingThreshold = [50]\n",
    "numSTD = [1]\n",
    "input_lengths = [13]\n",
    "horizon_lengths = [24]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70173961",
   "metadata": {},
   "source": [
    "# Data/Model Creation, Model Training, Model Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd81d0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17236, 48, 13)\n",
      "(16198, 48, 13)\n",
      "Creating input and output vectors for Input: 13 Horizon: 24\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.5773\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.8631\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.5164\n",
      "KEYS IN TEST DATASET\n",
      "(13, 24)\n",
      "Creating Directory for R2\n",
      "(17236, 48, 13)\n",
      "(16198, 48, 13)\n",
      "Creating input and output vectors for Input: 13 Horizon: 24\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.8385\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 1.0124\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.5438\n",
      "KEYS IN TEST DATASET\n",
      "(13, 24)\n",
      "Creating Directory for R2\n",
      "(17236, 48, 13)\n",
      "(16198, 48, 13)\n",
      "Creating input and output vectors for Input: 13 Horizon: 24\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.9118\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.5041\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.5731\n",
      "KEYS IN TEST DATASET\n",
      "(13, 24)\n",
      "Creating Directory for R2\n",
      "(17236, 48, 13)\n",
      "(16198, 48, 13)\n",
      "Creating input and output vectors for Input: 13 Horizon: 24\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.6627\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 1.8125\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.4949\n",
      "KEYS IN TEST DATASET\n",
      "(13, 24)\n",
      "Creating Directory for R2\n",
      "(17236, 48, 13)\n",
      "(16198, 48, 13)\n",
      "Creating input and output vectors for Input: 13 Horizon: 24\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.6374\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.8584\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 1.0203\n",
      "KEYS IN TEST DATASET\n",
      "(13, 24)\n",
      "Creating Directory for R2\n",
      "(17236, 48, 13)\n",
      "(16198, 48, 13)\n",
      "Creating input and output vectors for Input: 13 Horizon: 24\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 1.3593\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.5311\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.5384\n",
      "KEYS IN TEST DATASET\n",
      "(13, 24)\n",
      "Creating Directory for R2\n",
      "(17236, 48, 13)\n",
      "(16198, 48, 13)\n",
      "Creating input and output vectors for Input: 13 Horizon: 24\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.9376\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 2.4254\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.6244\n",
      "KEYS IN TEST DATASET\n",
      "(13, 24)\n",
      "Creating Directory for R2\n",
      "(17236, 48, 13)\n",
      "(16198, 48, 13)\n",
      "Creating input and output vectors for Input: 13 Horizon: 24\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.6455\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.7602\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.6258\n",
      "KEYS IN TEST DATASET\n",
      "(13, 24)\n",
      "Creating Directory for R2\n",
      "(17236, 48, 13)\n",
      "(16198, 48, 13)\n",
      "Creating input and output vectors for Input: 13 Horizon: 24\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 1.0449\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.7840\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.5715\n",
      "KEYS IN TEST DATASET\n",
      "(13, 24)\n",
      "Creating Directory for R2\n",
      "(17236, 48, 13)\n",
      "(16198, 48, 13)\n",
      "Creating input and output vectors for Input: 13 Horizon: 24\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.5839\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.5122\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.7552\n",
      "KEYS IN TEST DATASET\n",
      "(13, 24)\n",
      "Creating Directory for R2\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(numSTD)):\n",
    "#     for in_length in range(len(input_lengths)):\n",
    "\n",
    "#         10 different train_test splits\n",
    "    for i in range(10):\n",
    "        input_length = input_lengths[index]\n",
    "        mT = maskingThreshold[index]\n",
    "        nstd = numSTD[index]\n",
    "        np_path = \"D:\\\\Final Dataset\\\\threshold_\" + str(mT) + \"_nSTD_\" + str(nstd) + \"\\\\np_array_no_text.npy\"\n",
    "        dataset = np.load(np_path)\n",
    "        train_dictionary, test_dataset, min_set_size = createInputOutputs(dataset, input_length, horizon_lengths, i, 0.2)\n",
    "        max_size = int(min_set_size * 1.5)\n",
    "        trimmed_train_dictionary = trimSets(train_dictionary, max_size, input_length)\n",
    "        merged_X, merged_y, x_lengths, y_lengths = mergeAllSets(trimmed_train_dictionary, input_length)\n",
    "        train_data_loader = createDataLoader(merged_X, merged_y, x_lengths, y_lengths)\n",
    "\n",
    "        model = AutoregressiveLSTMWithAttention(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "        trainModel()\n",
    "        r2s = evaluateModel()\n",
    "        saveR2Dict(mT, nstd, input_length, i)\n",
    "        \n",
    "np.load = np_load_old\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79f047",
   "metadata": {},
   "source": [
    "# Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26a2adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(r2s[13,12][\"by_column\"])\n",
    "# print(r2s[13,12][\"global\"])\n",
    "\n",
    "# input_lengths = [8, 2, 4, 5, 18, 24, 15, 14, 13, 17, 23, 21, 20, 7, 1, 10, 11]\n",
    "# horizon_lengths = [1, 6, 12, 24]\n",
    "# maskingThreshold = [150, 75, 94, 113, 144, 81, 69, 138, 100, 50, 125, 106, 88, 56, 119, 131, 63]\n",
    "# numSTD = [1.8, 2.1, 1.5, 3, 1.3, 2.6, 1.6, 2.9, 2, 2.3, 1.9, 2.5, 1, 2.8, 1.4, 2.4, 1.1]\n",
    "\n",
    "# maskingThreshold = [46, 17, 24, 32, 96, 125, 82, 75, 68, 89, 118, 111, 103, 39, 10, 53, 60]\n",
    "# numSTD = [1.5, 0.5, 0.7, 1, 1.4, 0.5, 0.4,1.3, 0.8, 0.1, 1.2, 0.9, 0.6, 0.2, 1.1, 1.2, 0.3]\n",
    "# input_lengths = [12, 13, 2, 5, 7, 6, 15, 12, 8, 4, 3, 14, 11, 9, 10, 1, 5]\n",
    "# horizon_lengths = [1, 6, 12, 24]\n",
    "\n",
    "# maskingThreshold = [58, 36, 41, 47, 98, 120, 86, 81, 75, 92, 114, 109, 103, 53, 30, 64, 69]\n",
    "# numSTD = [1.5, 0.8, 0.9, 1.1, 1.4, 0.8, 0.7, 1.4, 1, 0.5, 1.3, 1.1, 0.9, 0.6, 1.2, 1.3, 0.6]\n",
    "# input_lengths = [13, 14, 4, 7, 8, 8, 15, 12, 9, 5, 5, 14, 11, 10, 11, 3, 6]\n",
    "# horizon_lengths = [1, 6, 12, 24]\n",
    "\n",
    "# maskingThreshold = [114]\n",
    "# numSTD = [1]\n",
    "# input_lengths = [8]\n",
    "# horizon_lengths = [1]\n",
    "\n",
    "# maskingThreshold = [1]\n",
    "# numSTD = [65]\n",
    "# input_lengths = [1, 2, 4, 8, 16, 24]\n",
    "# horizon_lengths = [6]\n",
    "\n",
    "\n",
    "# index = 4\n",
    "# mT = maskingThreshold[index]\n",
    "# nstd = numSTD[index]\n",
    "# input_length = input_lengths[index]\n",
    "# splitn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87d5ad8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_60796\\3485839178.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mloaded_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mfirst8_horz1\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloaded_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhorizon_lengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'first8'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;31m#             first8_horz6 += loaded_dict[input_length, 6]['first8']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#             first8_horz12 += loaded_dict[input_length, 12]['first8']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "for index in range(len(numSTD)):\n",
    "    results_dict = {}\n",
    "    \n",
    "    first8_horz1 = 0\n",
    "    first8_horz6 = 0\n",
    "    first8_horz12 = 0\n",
    "    first8_horz24 = 0\n",
    "    \n",
    "    total_horz1 = 0\n",
    "    total_horz6 = 0\n",
    "    total_horz12 = 0\n",
    "    total_horz24 = 0\n",
    "    \n",
    "    for splitn in range(10):\n",
    "        input_length = input_lengths[index]\n",
    "        mT = maskingThreshold[index]\n",
    "        nstd = numSTD[index]\n",
    "        dict_path = \"C:\\\\Users\\\\jared\\\\OneDrive\\\\Documents\\\\School\\\\Thesis\\\\Final Results\\\\threshold_\" + str(mT) + \"_nSTD_\" + str(nstd) + \"\\\\input_\" + str(input_length) + \"\\\\train_test_split_\" + str(splitn) + \"_r2_dict.pkl\"\n",
    "        key = index\n",
    "        with open(dict_path, 'rb') as file:\n",
    "            loaded_dict = pickle.load(file)\n",
    "            \n",
    "            first8_horz1 += loaded_dict[input_length, horizon_lengths[0]]['first8']\n",
    "#             first8_horz6 += loaded_dict[input_length, 6]['first8']\n",
    "#             first8_horz12 += loaded_dict[input_length, 12]['first8']\n",
    "#             first8_horz24 += loaded_dict[input_length, 24]['first8']\n",
    "            \n",
    "            total_horz1 += loaded_dict[input_length, horizon_lengths[0]]['global']\n",
    "#             total_horz6 += loaded_dict[input_length, 6]['global']\n",
    "#             total_horz12 += loaded_dict[input_length, 12]['global']\n",
    "#             total_horz24 += loaded_dict[input_length, 24]['global']\n",
    "            \n",
    "            \n",
    "    \n",
    "    first8_horz1 = first8_horz1/10\n",
    "    first8_horz6 = first8_horz6/10\n",
    "    first8_horz12 = first8_horz12/10\n",
    "    first8_horz24 = first8_horz24/10\n",
    "    \n",
    "    total_horz1 = total_horz1/10\n",
    "    total_horz6 = total_horz6/10\n",
    "    total_horz12 = total_horz12/10\n",
    "    total_horz24 = total_horz24/10\n",
    "    \n",
    "    \n",
    "    key = (mT, nstd, input_length, horizon_lengths[0])\n",
    "    results_dict[key] = {'first8': first8_horz1, 'global': total_horz1}\n",
    "        \n",
    "#     key = (mT, nstd, input_length, 6)\n",
    "#     results_dict[key] = {'first8': first8_horz6, 'global': total_horz6}\n",
    "        \n",
    "#     key = (mT, nstd, input_length, 12)\n",
    "#     results_dict[key] = {'first8': first8_horz12, 'global': total_horz12}\n",
    "        \n",
    "#     key = (mT, nstd, input_length, 24)\n",
    "#     results_dict[key] = {'first8': first8_horz24, 'global': total_horz24}\n",
    "    \n",
    "    save_path = \"C:\\\\Users\\\\jared\\\\OneDrive\\\\Documents\\\\School\\\\Thesis\\\\Final Results\\\\threshold_\" + str(mT) + \"_nSTD_\" + str(nstd) + \"\\\\input_\" + str(input_length) + \"\\\\Aggregate_Results_Dict.pkl\"\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(results_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b5fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_aggregate_optimization_data = []\n",
    "\n",
    "for index in range(len(numSTD)):\n",
    "    \n",
    "    mT = maskingThreshold[index]\n",
    "    nstd = numSTD[index]\n",
    "    input_length = input_lengths[index]\n",
    "    \n",
    "#     hyper_params = (mT, nstd, input_length)\n",
    "    hyper_params = (mT, nstd)\n",
    "    \n",
    "    dict_path = \"C:\\\\Users\\\\jared\\\\OneDrive\\\\Documents\\\\School\\\\Thesis\\\\Final Results\\\\threshold_\" + str(mT) + \"_nSTD_\" + str(nstd) + \"\\\\input_\" + str(input_length) + \"\\\\Aggregate_Results_Dict.pkl\"\n",
    "    with open(dict_path, 'rb') as file:\n",
    "        loaded_dict = pickle.load(file)\n",
    "\n",
    "    for key, value in loaded_dict.items():\n",
    "        flattened_row = list(key) + [value[\"first8\"], value[\"global\"]]\n",
    "#         flattened_row = list(key) + [value[\"first8\"]]\n",
    "        total_aggregate_optimization_data.append(flattened_row)\n",
    "\n",
    "        \n",
    "df = pd.DataFrame(total_aggregate_optimization_data, columns=['maskThreshold', 'nSTD', 'input_length', 'horizon_length', 'first8r2', 'globalr2'])\n",
    "# df = pd.DataFrame(total_aggregate_optimization_data, columns=['maskThreshold', 'nSTD', 'input_length', 'horizon_length', 'first8r2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:\\\\Users\\\\jared\\\\OneDrive\\\\Documents\\\\School\\\\Thesis\\\\Final Results\\\\Aggregate_Results_For_Optimization.csv'\n",
    "\n",
    "# Save the DataFrame to CSV at the specified location\n",
    "df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb853d2",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5018446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df[df[\"horizon_length\"] != 1]\n",
    "\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa736b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_grouped = df.groupby(np.arange(len(df)) // 4).mean()\n",
    "# df_grouped = df_grouped.drop(columns=['horizon_length'])\n",
    "\n",
    "# print(df_grouped)\n",
    "\n",
    "# file_path = 'C:\\\\Users\\\\jared\\\\OneDrive\\\\Documents\\\\School\\\\Thesis\\\\Final Results\\\\mt_nstd_input_first8.csv'\n",
    "\n",
    "# # Save the DataFrame to CSV at the specified location\n",
    "# df_grouped.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d70d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f734b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_lengths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cf6c5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYS IN TEST DATASET\n",
      "(13, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2594,\n",
       " 0.4336,\n",
       " 0.4237,\n",
       " 0.2035,\n",
       " 0.1824,\n",
       " 0.5564,\n",
       " 0.5373,\n",
       " 0.4768,\n",
       " 0.0002,\n",
       " 0.0015,\n",
       " 0.0053,\n",
       " 0.1389,\n",
       " -0.0006]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_col = evaluateModel()\n",
    "r2_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86455a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82e109",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66360b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for horizon 24 was used to create plots\n",
    "index = 12\n",
    "features = [\"Semi-Major Axis Length\", \"Semi-Minor Axis Length\", \"Ellipse Area\", \"Axis Ratio\", \"Ellipse Angle\", \"Center of Mass X Coordinate\", \"Center of Mass Y Coordinate\", \"Non-Zero Pixel Average\", \"Average\", \"X Velocity\", \"Y Velocity\", \"Velocity Magnitude\", \"Angular Velocity\"]\n",
    "y = dataset[100][:,index]\n",
    "x = 5*np.arange(y.size)\n",
    "plt.plot(x, y)\n",
    "plt.title(features[index] + \" Over time\")\n",
    "plt.xlabel(\"Time (minutes)\")\n",
    "plt.ylabel(\"Feature Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maskingThreshold = [114,  65,   50,   50]\n",
    "# numSTD =           [1,    1,    1,    1]\n",
    "# input_lengths =    [8,    4,    9,    13]\n",
    "# horizon_lengths =  [1,    6,    12,   24]\n",
    "\n",
    "mT = 114\n",
    "nstd = 1\n",
    "# input_lengths = [9]\n",
    "# horizon_lengths = [12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a891ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "np_path = \"D:\\\\Final Dataset\\\\threshold_\" + str(mT) + \"_nSTD_\" + str(nstd) + \"\\\\np_array_no_text.npy\"\n",
    "dataset = np.load(np_path)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f64c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f96bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
