{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b11c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py # needs conda/pip install h5py\n",
    "# import os\n",
    "# os.chdir(r'C:\\Users\\jared\\OneDrive\\Documents\\School\\Thesis')\n",
    "\n",
    "# import sys\n",
    "# sys.path.append(r'C:\\users\\jared\\anaconda3\\lib\\site-packages')\n",
    "# # import command_file\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import spear as sp\n",
    "# import numpy as np\n",
    "# from pathlib import Path  \n",
    "# import time\n",
    "# import math\n",
    "# import os\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # import gruHelperScript as ghs\n",
    "# # import lstmHelperScript as lhs\n",
    "# # import encoderDecoderHelperScript as edhs\n",
    "\n",
    "# DATA_PATH    = 'D:\\SEVIR Data\\data'\n",
    "# CATALOG_PATH = 'D:\\SEVIR Data/CATALOG.csv'\n",
    "\n",
    "# # Read catalog\n",
    "# catalog = pd.read_csv(CATALOG_PATH,parse_dates=['time_utc'],low_memory=False)\n",
    "\n",
    "# # Desired image types\n",
    "# img_types = set(['vil'])\n",
    "\n",
    "# # Group by event id, and filter to only events that have all desired img_types\n",
    "# events = catalog.groupby('id').filter(lambda x: img_types.issubset(set(x['img_type']))).groupby('id')\n",
    "# event_ids = list(events.groups.keys())\n",
    "# print('Found %d events matching' % len(event_ids),img_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ccfb04",
   "metadata": {},
   "source": [
    "# CHANGE FOR LOOP BELOW TO DO ALL FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a238da5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20273 events matching {'vil'}\n"
     ]
    }
   ],
   "source": [
    "import master_script_helper_functions as mshf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9714f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58ccbfef",
   "metadata": {},
   "source": [
    "# Creating X and Y For and Pytorch Data Loaders for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a315a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createXY(dataset, input_length, horizon_length):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sample in dataset:\n",
    "        for i in range(len(sample) - input_length - horizon_length + 1):\n",
    "            x = torch.tensor(sample[i:i+input_length], dtype=torch.float32)\n",
    "            y = torch.tensor(sample[i+input_length:i+input_length+horizon_length], dtype=torch.float32)\n",
    "#             x_padded = pad_sequence(x, max_input_len, 0)\n",
    "#             y_padded = pad_sequence(y, max_horizon_len, 0)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10a9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataset(train, test):\n",
    "    stacked_train = np.array(np.vstack(train))\n",
    "    stacked_train = stacked_train.astype(np.float64)\n",
    "    means = np.mean(stacked_train, axis=0)\n",
    "    stds =  np.std(stacked_train, axis=0)\n",
    "    normalized_train = (stacked_train - means) / stds\n",
    "    train_return = normalized_train.reshape(-1,48,13)\n",
    "    \n",
    "    stacked_test = np.array(np.vstack(test))\n",
    "#     stacked_test = stacked_test.astype(np.float64)\n",
    "    normalized_test = (stacked_test - means) / stds\n",
    "    test_return = normalized_test.reshape(-1,48,13)\n",
    "    return train_return, test_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9180e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeZeroEvents(dataset):\n",
    "    print(dataset.shape)\n",
    "    \n",
    "    dataset = dataset.astype(np.float64)\n",
    "    valid_events_mask = ~np.any(np.isnan(dataset), axis=(1, 2))\n",
    "    # Use the mask to filter out the invalid events\n",
    "    filtered_data = dataset[valid_events_mask]\n",
    "    \n",
    "    new_set = []\n",
    "    for event in filtered_data:\n",
    "        if not(np.any(event[:, 0:2] < 3)):\n",
    "            new_set.append(event)\n",
    "    new_set = np.array(new_set)\n",
    "    new_set = new_set.astype(np.float64)\n",
    "#     valid_events_mask = ~np.any(np.isnan(new_set), axis=(1, 2))\n",
    "#     # Use the mask to filter out the invalid events\n",
    "#     filtered_data = new_set[valid_events_mask]\n",
    "    \n",
    "    print(new_set.shape)\n",
    "    return new_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee0dbb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInputOutputs(dataset, input_length, output_lengths, rand_state, test_percent=0.2):\n",
    "    \"\"\"    \n",
    "    Inputs: Dataset, input_lengths, output_lengths (horizons), test_set_percentage (20% default)\n",
    "    \n",
    "    Removes zero events\n",
    "    Splits into train/test (80/20 ratio default)\n",
    "    Normalizes train, then normalizes test using train means\n",
    "    Creates dictionary of X/Y vectors for each input/output combo\n",
    "    Returns dictionary of X/Y vectors for both train and test set\n",
    "    \n",
    "    Returns: normalized dictionary of X/Y Vectors for every input/output combo\n",
    "    \"\"\"\n",
    "    dataset = removeZeroEvents(dataset)\n",
    "#     Split intro train/test with default 20% test and then normalize\n",
    "    train, test = train_test_split(dataset, test_size=test_percent, random_state=rand_state)\n",
    "    norm_train, norm_test = normalizeDataset(train, test)\n",
    "    \n",
    "    train_dict = {}\n",
    "    test_dict = {}\n",
    "    min_set_size = 20000\n",
    "#     Create dictionary with keys of (input_length, horizon_length)\n",
    "#     Each keys has a value with the X vectors as [\"X\"] and y vectors as [\"y\"]\n",
    "#     for input_length in input_lengths:\n",
    "    for horizon_length in horizon_lengths:\n",
    "        print(\"Creating input and output vectors for Input: \" + str(input_length) + \" Horizon: \" + str(horizon_length))\n",
    "        train_x, train_y = createXY(norm_train, input_length, horizon_length)\n",
    "        key = (input_length, horizon_length)  # Create a key for the combination\n",
    "        train_dict[key] = {\"X\": [], \"Y\": []}\n",
    "        train_dict[key][\"X\"].append(train_x)\n",
    "        train_dict[key][\"Y\"].append(train_y)\n",
    "\n",
    "        test_x, test_y = createXY(norm_test, input_length, horizon_length)\n",
    "#         key = (input_length, horizon_length)  # Create a key for the combination\n",
    "        test_dict[key] = {\"X\": [], \"Y\": []}\n",
    "        test_dict[key][\"X\"].append(test_x)\n",
    "        test_dict[key][\"Y\"].append(test_y)\n",
    "        if(len(train_x) < min_set_size):\n",
    "            min_set_size = len(train_x)\n",
    "    return train_dict, test_dict, min_set_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c137e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X_data, Y_data, X_lengths, Y_lengths):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "        self.X_lengths = X_lengths\n",
    "        self.Y_lengths = Y_lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X_data[idx]\n",
    "        Y = self.Y_data[idx]\n",
    "        X_len = self.X_lengths[idx]\n",
    "        Y_len = self.Y_lengths[idx]\n",
    "        \n",
    "        return X, Y, X_len, Y_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a6d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate_fn to pad X and unpack Y sequences\n",
    "def collate_fn(batch):\n",
    "    # Sort by length in descending order (for packing)\n",
    "    batch.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    X, Y, X_lengths, Y_lengths = zip(*batch)\n",
    "    \n",
    "    # Pad X sequences\n",
    "    X_padded = pad_sequence(X, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Stack Y sequences (no padding here, since Y is unpadded)\n",
    "    Y_padded = pad_sequence(Y, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    X_lengths = torch.tensor(X_lengths)\n",
    "    Y_lengths = torch.tensor(Y_lengths)\n",
    "\n",
    "    return X_padded, Y_padded, X_lengths, Y_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c611a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimSets(dictionary, max_size, input_length):\n",
    "#     Equalize size of each input/horizon pairing in train dictionary (max set size is 1.5x smallest set size)\n",
    "    for horizon_length in horizon_lengths:\n",
    "        X = dictionary[(input_length, horizon_length)][\"X\"][0]\n",
    "        y = dictionary[(input_length, horizon_length)][\"Y\"][0]\n",
    "\n",
    "        if(len(X) > max_size):\n",
    "            random_indices = random.sample(range(len(X)), max_size)\n",
    "            # Slice the lists using the random indices\n",
    "            X_Sampled = [X[i] for i in random_indices]\n",
    "            y_Sampled = [y[i] for i in random_indices]\n",
    "\n",
    "            dictionary[(input_length, horizon_length)][\"X\"][0] = X_Sampled\n",
    "            dictionary[(input_length, horizon_length)][\"Y\"][0] = y_Sampled\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f047e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create merged X Y for train dataset\n",
    "def mergeAllSets(dictionary, input_len):\n",
    "    X_all_Train = []\n",
    "    Y_all_Train = []\n",
    "    # total = 0\n",
    "    train_x_lengths = []\n",
    "    train_y_lengths = []\n",
    "\n",
    "    for horizon_length in horizon_lengths:\n",
    "        # Get the data for the current (input_len, output_horizon) pair\n",
    "        key = (input_len, horizon_length)\n",
    "        X = dictionary[key][\"X\"][0]\n",
    "        Y = dictionary[key][\"Y\"][0]\n",
    "        X_all_Train = X_all_Train + X\n",
    "        Y_all_Train = Y_all_Train + Y\n",
    "        train_x_lengths = train_x_lengths + [input_len]*len(dictionary[key][\"X\"][0])\n",
    "        train_y_lengths = train_y_lengths + [horizon_length]*len(dictionary[key][\"Y\"][0])\n",
    "        \n",
    "    return X_all_Train, Y_all_Train, train_x_lengths, train_y_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae7e8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataLoader(X_all_Train, Y_all_Train, train_x_lengths, train_y_lengths):\n",
    "    training_torch_dataset = SeqDataset(X_all_Train, Y_all_Train, train_x_lengths, train_y_lengths)\n",
    "\n",
    "#     For testing purposes, take 20% of dataset\n",
    "    dataset_size = len(training_torch_dataset) \n",
    "    subset_size = int(0.2 * dataset_size)  # Taking 20% of the dataset\n",
    "    subset, _ = random_split(training_torch_dataset, [subset_size, dataset_size - subset_size])\n",
    "    \n",
    "    train_data_loader = DataLoader(training_torch_dataset, batch_size=256, shuffle=True, drop_last=True, \n",
    "                                   collate_fn=lambda batch: collate_fn(batch))\n",
    "    return train_data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1aee801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel():\n",
    "    criterion = nn.MSELoss()  # Assuming you are doing regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training model\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, (X_batch, Y_batch, X_lengths, Y_lengths) in enumerate(train_data_loader):  # Assuming a data loader is used\n",
    "\n",
    "            # Calculate percentage\n",
    "            percent_done = (i + 1) / len(train_data_loader) * 100\n",
    "            # Print progress\n",
    "            print(f\"Processing: {percent_done:.2f}% complete for epoch {epoch+1}\", end='\\r')\n",
    "\n",
    "\n",
    "            # Move the batch to the same device as the model\n",
    "            X_batch = X_batch.to(device)\n",
    "            Y_batch = Y_batch.to(device)\n",
    "            X_lengths = X_lengths.to(device)\n",
    "            Y_lengths = Y_lengths.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            Y_pred = model(X_batch, X_lengths, Y_lengths.max())\n",
    "\n",
    "            # Compute loss on non-padded parts\n",
    "            trimmed_y_pred = [arr[n-1] for arr, n in zip(Y_pred, Y_lengths)]\n",
    "            trimmed_y_true = [arr[n-1] for arr, n in zip(Y_batch, Y_lengths)]\n",
    "            trimmed_y_pred_tensor = torch.stack(trimmed_y_pred)\n",
    "            trimmed_y_true_tensor = torch.stack(trimmed_y_true)\n",
    "\n",
    "        #         print(trimmed_y_pred[0])\n",
    "        #         print(trimmed_y_true[0])\n",
    "#             loss = criterion(trimmed_y_pred_tensor, trimmed_y_true_tensor)\n",
    "            loss = criterion(trimmed_y_pred_tensor[:,:-5], trimmed_y_true_tensor[:,:-5])\n",
    "\n",
    "            # Zero the gradients, perform a backward pass, and update the weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print loss\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96785173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class AutoregressiveLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.2):\n",
    "        super(AutoregressiveLSTMWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer for processing inputs\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, \n",
    "                             num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Fully connected layer to project hidden states to output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Attention Layer\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths, output_length):\n",
    "        # Pack the padded sequence (for training only)\n",
    "        packed_input = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass the packed input through LSTM\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
    "        \n",
    "        # Unpack the output (for training only, not used in autoregressive generation)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        # Now use hidden state to start autoregressive generation\n",
    "        next_input = output[:, -1, :].unsqueeze(1)  # Last hidden state as input\n",
    "        # Initialize hidden and cell states\n",
    "        next_input = self.fc(next_input.squeeze(1)).unsqueeze(1)  # Shape: [batch_size, 1, input_size]\n",
    "\n",
    "        hidden = hidden  # [num_layers, batch_size, hidden_size]\n",
    "        cell = cell  # [num_layers, batch_size, hidden_size]\n",
    "        \n",
    "        outputs = []\n",
    "        past_predictions = []  # Store past predictions for attention\n",
    "        \n",
    "        for t in range(output_length):\n",
    "            # LSTM generates a new output\n",
    "#             print(\"LSTM INPUT SIZE: \" + str(x.shape))\n",
    "            output_lstm, (hidden, cell) = self.lstm(next_input, (hidden, cell))\n",
    "#             print(\"OUTPUT_LSTM SIZE: \" + str(output_lstm.shape))\n",
    "\n",
    "            # Use attention on past predictions\n",
    "            if past_predictions:\n",
    "                past_preds = torch.stack(past_predictions, dim=1)  # [batch_size, t, output_size]\n",
    "                attn_weights = F.softmax(self.attn(past_preds), dim=1)  # Attention weights for past preds\n",
    "                weighted_past_preds = torch.sum(attn_weights * past_preds, dim=1)  # Weighted sum of past predictions\n",
    "#                 print(\"WEIGHTED PAST PREDS: \" + str(weighted_past_preds.shape))\n",
    "                output_lstm = output_lstm + weighted_past_preds.squeeze(1)  # Combine with new LSTM output\n",
    "#                 print(\"OUTPUT_LSTM SIZE w attn: \" + str(output_lstm.shape))\n",
    "                \n",
    "            # Fully connected layer for final prediction\n",
    "            output = self.fc(output_lstm.squeeze(1))  # [batch_size, output_size]\n",
    "            outputs.append(output)\n",
    "            \n",
    "            # Save the current prediction for future attention\n",
    "            past_predictions.append(output_lstm.unsqueeze(1))  # Add to past predictions list\n",
    "            \n",
    "            # Use output as next input for autoregressive generation\n",
    "            next_input = output.unsqueeze(1)  # This will be the sequence input for next timestep\n",
    "#             print(\"OUTPUT SIZE: \" + str(output.shape))\n",
    "\n",
    "        return torch.stack(outputs, dim=1)  # Stack to form a sequence of shape [batch_size, output_length, output_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a8dde64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTestSetDataloader(key):\n",
    "    current_x_test = test_dataset[key][\"X\"][0]\n",
    "    current_y_test = test_dataset[key][\"Y\"][0]\n",
    "    current_x_test_lengths = [key[0]] * len(current_x_test)\n",
    "    current_y_test_lengths = [key[1]] * len(current_y_test)\n",
    "    current_test_dataset = SeqDataset(current_x_test, current_y_test, current_x_test_lengths, current_y_test_lengths)\n",
    "    test_data_loader = DataLoader(current_test_dataset, batch_size=128, shuffle=True, drop_last=True, collate_fn=lambda batch: collate_fn(batch))\n",
    "    return test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89f0b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2ByColumn(y_true, y_pred):\n",
    "    # Calculate the mean of y_true along each column\n",
    "    y_mean = np.mean(y_true, axis=0)\n",
    "    # Calculate SS_tot and SS_res for each column\n",
    "    ss_tot = np.sum((y_true - y_mean) ** 2, axis=0)\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2, axis=0)\n",
    "    # Calculate R^2 for each column\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    rounded_r2 = [np.round(x, 4) for x in r2]\n",
    "    return rounded_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7491567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def globalR2(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true, axis=0)) ** 2)\n",
    "    r2_global = 1 - (ss_res / ss_tot)\n",
    "    return r2_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2607a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel():\n",
    "    r2_dict = {}\n",
    "    for key in test_dataset:\n",
    "        print(\"KEYS IN TEST DATASET\")\n",
    "        print(key)\n",
    "        test_data_loader = makeTestSetDataloader(key)\n",
    "        predictions = []\n",
    "        y_true = []\n",
    "        for i, (X_batch, Y_batch, X_lengths, Y_lengths) in enumerate(test_data_loader):\n",
    "            with torch.no_grad():\n",
    "                batch_predictions = model(X_batch.to(device), X_lengths.to(device), Y_lengths.max().to(device))\n",
    "\n",
    "            numpy_batch_predictions = batch_predictions.cpu().numpy()\n",
    "    #         print(numpy_batch_predictions[0,:,:])\n",
    "            numpy_batch_predictions = numpy_batch_predictions[:, -1, :] # Take only the final frame for computing r^2\n",
    "\n",
    "    #         print(numpy_batch_predictions.shape)\n",
    "            predictions.append(numpy_batch_predictions)\n",
    "            batch_y_true = Y_batch[:, -1, :] # Take only the final frame for computing r^2\n",
    "            y_true.append(batch_y_true.cpu().numpy())\n",
    "\n",
    "        predictions_array = np.array(predictions)\n",
    "    #     print(\"INPUT LENGTH: \" + str(int(X_lengths[0])))\n",
    "    #     print(\"HORIZON LENGTH: \" + str(int(Y_lengths[0])))\n",
    "    #     print(\"Predictions Length: \" + str(predictions_array.shape))\n",
    "\n",
    "    #     predictions_array = predictions_array.reshape(-1, Y_lengths[0], output_size)\n",
    "        predictions_array = predictions_array.reshape(-1, 1, output_size)\n",
    "        predictions_array = np.squeeze(predictions_array)\n",
    "    #     print(\"Number of Samples: \" + str(predictions_array.shape[0]) + \"\\n\")\n",
    "\n",
    "        y_true = np.array(y_true)\n",
    "    #     y_true = y_true.reshape(-1, Y_lengths[0], output_size)\n",
    "        y_true = y_true.reshape(-1, 1, output_size)\n",
    "        y_true = np.squeeze(y_true)\n",
    "    #     print(\"y_true Length: \" + str(y_true.shape))\n",
    "\n",
    "#         r2_column = r2ByColumn(y_true, predictions_array)\n",
    "        r2_first8 = globalR2(y_true[:,:-5], predictions_array[:,:-5])\n",
    "        r2_global = globalR2(y_true, predictions_array)\n",
    "\n",
    "        r2_dict[key] = {\"first8\": r2_first8, \"global\": r2_global}\n",
    "    return r2_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49ba9edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveR2Dict(mT, nstd, input_length, splitn):\n",
    "    np_path = \"C:\\\\Users\\\\jared\\\\OneDrive\\\\Documents\\\\School\\\\Thesis\\\\Final Results\\\\threshold_\" + str(mT) + \"_nSTD_\" + str(nstd) + \"\\\\cluster1\\\\input_\" + str(input_length) + \"\\\\train_test_split_\" + str(splitn) + \"_r2_dict.pkl\"\n",
    "\n",
    "    # Extract the directory from the path (so we can create it)\n",
    "    directory = os.path.dirname(np_path)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    try:\n",
    "        print(\"Creating Directory for R2\")\n",
    "        os.makedirs(directory, exist_ok=True)  # exist_ok=True to prevent error if directory already exists\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating the directory: {e}\")\n",
    "\n",
    "    # Save the dictionary to the file using pickle\n",
    "    with open(np_path, 'wb') as f:\n",
    "        pickle.dump(r2s, f)  # Save the dictionary (r2s) into the .pkl file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106f230",
   "metadata": {},
   "source": [
    "# DEFINING GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cd4f7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import random\n",
    "from itertools import product\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f14d6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Params\n",
    "input_size = 13  # Feature dimension for each time step in X\n",
    "output_size = 13  # Feature dimension for each time step in Y (same as input size)\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d2f550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "maskingThreshold = [114, 114, 114, 114, 114, 114]\n",
    "numSTD = [1, 1, 1, 1, 1, 1]\n",
    "input_lengths = [1, 2, 4 ,8 ,16, 24]\n",
    "horizon_lengths = [1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70173961",
   "metadata": {},
   "source": [
    "# Data/Model Creation, Model Training, Model Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd81d0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 1 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.4836\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1556\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1112\n",
      "KEYS IN TEST DATASET\n",
      "(1, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 1 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.4571\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1298\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0810\n",
      "KEYS IN TEST DATASET\n",
      "(1, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 1 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.3690\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1912\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1573\n",
      "KEYS IN TEST DATASET\n",
      "(1, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 1 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.4101\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1943\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0917\n",
      "KEYS IN TEST DATASET\n",
      "(1, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 1 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.4350\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1319\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0882\n",
      "KEYS IN TEST DATASET\n",
      "(1, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 1 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.6659\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1421\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1271\n",
      "KEYS IN TEST DATASET\n",
      "(1, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 1 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.4856\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1691\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1106\n",
      "KEYS IN TEST DATASET\n",
      "(1, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 1 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.3587\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1638\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1111\n",
      "KEYS IN TEST DATASET\n",
      "(1, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 1 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.3257\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1222\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0981\n",
      "KEYS IN TEST DATASET\n",
      "(1, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 1 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.4005\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.2738\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1246\n",
      "KEYS IN TEST DATASET\n",
      "(1, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 2 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2334\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1517\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1019\n",
      "KEYS IN TEST DATASET\n",
      "(2, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 2 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1883\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1886\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0973\n",
      "KEYS IN TEST DATASET\n",
      "(2, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 2 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2692\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1169\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0949\n",
      "KEYS IN TEST DATASET\n",
      "(2, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 2 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2916\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1573\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1344\n",
      "KEYS IN TEST DATASET\n",
      "(2, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 2 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.3425\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1124\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1462\n",
      "KEYS IN TEST DATASET\n",
      "(2, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 2 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2197\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1122\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1145\n",
      "KEYS IN TEST DATASET\n",
      "(2, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 2 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2727\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1404\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1187\n",
      "KEYS IN TEST DATASET\n",
      "(2, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 2 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2396\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1505\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1048\n",
      "KEYS IN TEST DATASET\n",
      "(2, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 2 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2611\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1591\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.3373\n",
      "KEYS IN TEST DATASET\n",
      "(2, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 2 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2559\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1193\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1088\n",
      "KEYS IN TEST DATASET\n",
      "(2, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 4 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2098\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1723\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1277\n",
      "KEYS IN TEST DATASET\n",
      "(4, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 4 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2261\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1100\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1095\n",
      "KEYS IN TEST DATASET\n",
      "(4, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 4 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1567\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1451\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1004\n",
      "KEYS IN TEST DATASET\n",
      "(4, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 4 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2160\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1608\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1122\n",
      "KEYS IN TEST DATASET\n",
      "(4, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 4 Horizon: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2537\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1977\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.2309\n",
      "KEYS IN TEST DATASET\n",
      "(4, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 4 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1888\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1422\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1368\n",
      "KEYS IN TEST DATASET\n",
      "(4, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 4 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1822\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1348\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0879\n",
      "KEYS IN TEST DATASET\n",
      "(4, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 4 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1835\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1185\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1422\n",
      "KEYS IN TEST DATASET\n",
      "(4, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 4 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2729\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1371\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1019\n",
      "KEYS IN TEST DATASET\n",
      "(4, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 4 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1581\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1402\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1508\n",
      "KEYS IN TEST DATASET\n",
      "(4, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 8 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2095\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1303\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1114\n",
      "KEYS IN TEST DATASET\n",
      "(8, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 8 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2642\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1513\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1370\n",
      "KEYS IN TEST DATASET\n",
      "(8, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 8 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1925\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1837\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1580\n",
      "KEYS IN TEST DATASET\n",
      "(8, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 8 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2060\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1605\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1427\n",
      "KEYS IN TEST DATASET\n",
      "(8, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 8 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2620\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1140\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.2543\n",
      "KEYS IN TEST DATASET\n",
      "(8, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 8 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2153\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1480\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0965\n",
      "KEYS IN TEST DATASET\n",
      "(8, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 8 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1991\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.0920\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0923\n",
      "KEYS IN TEST DATASET\n",
      "(8, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 8 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1908\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1569\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0989\n",
      "KEYS IN TEST DATASET\n",
      "(8, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 8 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2140\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.2347\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0994\n",
      "KEYS IN TEST DATASET\n",
      "(8, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 8 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.3124\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1588\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1226\n",
      "KEYS IN TEST DATASET\n",
      "(8, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 16 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1891\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1480\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1695\n",
      "KEYS IN TEST DATASET\n",
      "(16, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 16 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2262\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1477\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0963\n",
      "KEYS IN TEST DATASET\n",
      "(16, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 16 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2158\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1122\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1054\n",
      "KEYS IN TEST DATASET\n",
      "(16, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 16 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2168\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1279\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1232\n",
      "KEYS IN TEST DATASET\n",
      "(16, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 16 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1670\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1465\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0886\n",
      "KEYS IN TEST DATASET\n",
      "(16, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 16 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1848\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1358\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0817\n",
      "KEYS IN TEST DATASET\n",
      "(16, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 16 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.3148\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1390\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0970\n",
      "KEYS IN TEST DATASET\n",
      "(16, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 16 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1783\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1643\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1178\n",
      "KEYS IN TEST DATASET\n",
      "(16, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 16 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1546\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0961\n",
      "KEYS IN TEST DATASET\n",
      "(16, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 16 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1774\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.3124\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.2418\n",
      "KEYS IN TEST DATASET\n",
      "(16, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 24 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2133\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1057\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1433\n",
      "KEYS IN TEST DATASET\n",
      "(24, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 24 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2119\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1609\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0934\n",
      "KEYS IN TEST DATASET\n",
      "(24, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 24 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2045\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1271\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1184\n",
      "KEYS IN TEST DATASET\n",
      "(24, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 24 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.1849\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1343\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1139\n",
      "KEYS IN TEST DATASET\n",
      "(24, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 24 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2616\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1115\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1231\n",
      "KEYS IN TEST DATASET\n",
      "(24, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 24 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2433\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1259\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1133\n",
      "KEYS IN TEST DATASET\n",
      "(24, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 24 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.3856\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1433\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.0931\n",
      "KEYS IN TEST DATASET\n",
      "(24, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 24 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.3927\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1248\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1177\n",
      "KEYS IN TEST DATASET\n",
      "(24, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 24 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2113\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.0944\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1128\n",
      "KEYS IN TEST DATASET\n",
      "(24, 1)\n",
      "Creating Directory for R2\n",
      "(10446, 48, 13)\n",
      "(9117, 48, 13)\n",
      "Creating input and output vectors for Input: 24 Horizon: 1\n",
      "Processing: 100.00% complete for epoch 1\n",
      "Epoch 1/3, Loss: 0.2776\n",
      "Processing: 100.00% complete for epoch 2\n",
      "Epoch 2/3, Loss: 0.1043\n",
      "Processing: 100.00% complete for epoch 3\n",
      "Epoch 3/3, Loss: 0.1525\n",
      "KEYS IN TEST DATASET\n",
      "(24, 1)\n",
      "Creating Directory for R2\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(numSTD)):\n",
    "#     for in_length in range(len(input_lengths)):\n",
    "\n",
    "#         10 different train_test splits\n",
    "    for i in range(10):\n",
    "        input_length = input_lengths[index]\n",
    "        mT = maskingThreshold[index]\n",
    "        nstd = numSTD[index]\n",
    "#         np_path = \"D:\\\\Final Dataset\\\\threshold_\" + str(mT) + \"_nSTD_\" + str(nstd) + \"\\\\np_array_no_text.npy\"\n",
    "        np_path = \"D:\\\\Final Dataset\\\\threshold_\" + str(mT) + \"_nSTD_1\\\\Clusters\\Cluster 1\\cluster1_arr.npy\"\n",
    "\n",
    "        dataset = np.load(np_path)\n",
    "        train_dictionary, test_dataset, min_set_size = createInputOutputs(dataset, input_length, horizon_lengths, i, 0.2)\n",
    "        max_size = int(min_set_size * 1.5)\n",
    "        trimmed_train_dictionary = trimSets(train_dictionary, max_size, input_length)\n",
    "        merged_X, merged_y, x_lengths, y_lengths = mergeAllSets(trimmed_train_dictionary, input_length)\n",
    "        train_data_loader = createDataLoader(merged_X, merged_y, x_lengths, y_lengths)\n",
    "\n",
    "        model = AutoregressiveLSTMWithAttention(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "        trainModel()\n",
    "        r2s = evaluateModel()\n",
    "        saveR2Dict(mT, nstd, input_length, i)\n",
    "        \n",
    "np.load = np_load_old\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79f047",
   "metadata": {},
   "source": [
    "# Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87d5ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(numSTD)):\n",
    "    results_dict = {}\n",
    "    \n",
    "    first8_horz1 = 0\n",
    "    first8_horz6 = 0\n",
    "    first8_horz12 = 0\n",
    "    first8_horz24 = 0\n",
    "    \n",
    "    total_horz1 = 0\n",
    "    total_horz6 = 0\n",
    "    total_horz12 = 0\n",
    "    total_horz24 = 0\n",
    "    \n",
    "    for splitn in range(10):\n",
    "        input_length = input_lengths[index]\n",
    "        mT = maskingThreshold[index]\n",
    "        nstd = numSTD[index]\n",
    "        dict_path = \"C:\\\\Users\\\\jared\\\\OneDrive\\\\Documents\\\\School\\\\Thesis\\\\Final Results\\\\threshold_\" + str(mT) + \"_nSTD_\" + str(nstd) + \"\\\\cluster1\\\\input_\" + str(input_length) + \"\\\\train_test_split_\" + str(splitn) + \"_r2_dict.pkl\"\n",
    "        key = index\n",
    "        with open(dict_path, 'rb') as file:\n",
    "            loaded_dict = pickle.load(file)\n",
    "            \n",
    "            first8_horz1 += loaded_dict[input_length, horizon_lengths[0]]['first8']\n",
    "#             first8_horz6 += loaded_dict[input_length, 6]['first8']\n",
    "#             first8_horz12 += loaded_dict[input_length, 12]['first8']\n",
    "#             first8_horz24 += loaded_dict[input_length, 24]['first8']\n",
    "            \n",
    "            total_horz1 += loaded_dict[input_length, horizon_lengths[0]]['global']\n",
    "#             total_horz6 += loaded_dict[input_length, 6]['global']\n",
    "#             total_horz12 += loaded_dict[input_length, 12]['global']\n",
    "#             total_horz24 += loaded_dict[input_length, 24]['global']\n",
    "            \n",
    "            \n",
    "    \n",
    "    first8_horz1 = first8_horz1/10\n",
    "    first8_horz6 = first8_horz6/10\n",
    "    first8_horz12 = first8_horz12/10\n",
    "    first8_horz24 = first8_horz24/10\n",
    "    \n",
    "    total_horz1 = total_horz1/10\n",
    "    total_horz6 = total_horz6/10\n",
    "    total_horz12 = total_horz12/10\n",
    "    total_horz24 = total_horz24/10\n",
    "    \n",
    "    \n",
    "    key = (mT, nstd, input_length, horizon_lengths[0])\n",
    "    results_dict[key] = {'first8': first8_horz1, 'global': total_horz1}\n",
    "        \n",
    "#     key = (mT, nstd, input_length, 6)\n",
    "#     results_dict[key] = {'first8': first8_horz6, 'global': total_horz6}\n",
    "        \n",
    "#     key = (mT, nstd, input_length, 12)\n",
    "#     results_dict[key] = {'first8': first8_horz12, 'global': total_horz12}\n",
    "        \n",
    "#     key = (mT, nstd, input_length, 24)\n",
    "#     results_dict[key] = {'first8': first8_horz24, 'global': total_horz24}\n",
    "    \n",
    "    save_path = \"C:\\\\Users\\\\jared\\\\OneDrive\\\\Documents\\\\School\\\\Thesis\\\\Final Results\\\\threshold_\" + str(mT) + \"_nSTD_\" + str(nstd) + \"\\\\cluster1\\\\input_\" + str(input_length) + \"\\\\Aggregate_Results_Dict.pkl\"\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(results_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42d6cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "829b5fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maskThreshold</th>\n",
       "      <th>nSTD</th>\n",
       "      <th>input_length</th>\n",
       "      <th>horizon_length</th>\n",
       "      <th>first8r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.874565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.878679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.879147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   maskThreshold  nSTD  input_length  horizon_length  first8r2\n",
       "0            114     1             1               1  0.874565\n",
       "1            114     1             2               1  0.878679\n",
       "2            114     1             4               1  0.876280\n",
       "3            114     1             8               1  0.880679\n",
       "4            114     1            16               1  0.882149\n",
       "5            114     1            24               1  0.879147"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_aggregate_optimization_data = []\n",
    "\n",
    "for index in range(len(numSTD)):\n",
    "    \n",
    "    mT = maskingThreshold[index]\n",
    "    nstd = numSTD[index]\n",
    "    input_length = input_lengths[index]\n",
    "    \n",
    "#     hyper_params = (mT, nstd, input_length)\n",
    "    hyper_params = (mT, nstd)\n",
    "    \n",
    "    dict_path = \"C:\\\\Users\\\\jared\\\\OneDrive\\\\Documents\\\\School\\\\Thesis\\\\Final Results\\\\threshold_\" + str(mT) + \"_nSTD_\" + str(nstd) + \"\\\\cluster1\\\\input_\" + str(input_length) + \"\\\\Aggregate_Results_Dict.pkl\"\n",
    "    with open(dict_path, 'rb') as file:\n",
    "        loaded_dict = pickle.load(file)\n",
    "\n",
    "    for key, value in loaded_dict.items():\n",
    "#         flattened_row = list(key) + [value[\"first8\"], value[\"global\"]]\n",
    "        flattened_row = list(key) + [value[\"first8\"]]\n",
    "        total_aggregate_optimization_data.append(flattened_row)\n",
    "\n",
    "        \n",
    "# df = pd.DataFrame(total_aggregate_optimization_data, columns=['maskThreshold', 'nSTD', 'input_length', 'horizon_length', 'first8r2', 'globalr2'])\n",
    "df = pd.DataFrame(total_aggregate_optimization_data, columns=['maskThreshold', 'nSTD', 'input_length', 'horizon_length', 'first8r2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bfc279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:\\\\Users\\\\jared\\\\OneDrive\\\\Documents\\\\School\\\\Thesis\\\\Final Results\\\\Cluster1\\\\Aggregate_Results_For_Optimization.csv'\n",
    "\n",
    "# Save the DataFrame to CSV at the specified location\n",
    "df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb853d2",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5018446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df[df[\"horizon_length\"] != 1]\n",
    "\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa736b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_grouped = df.groupby(np.arange(len(df)) // 4).mean()\n",
    "# df_grouped = df_grouped.drop(columns=['horizon_length'])\n",
    "\n",
    "# print(df_grouped)\n",
    "\n",
    "# file_path = 'C:\\\\Users\\\\jared\\\\OneDrive\\\\Documents\\\\School\\\\Thesis\\\\Final Results\\\\mt_nstd_input_first8.csv'\n",
    "\n",
    "# # Save the DataFrame to CSV at the specified location\n",
    "# df_grouped.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d70d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
